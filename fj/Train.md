# AI 训练调参实战笔记：利用 CPU/GPU 异构算力加速训练

本文档记录了在 **RTX 5090 (32GB) + 25 Core Xeon CPU** 环境下训练 **PFLD_GhostOne** 项目时的参数调优心得。

通过几轮迭代，我们将训练速度从 **单 Epoch 9 分钟** 优化至 **5 分 30 秒** (提速接近 40%)，并将硬件性能发挥到极致。

---

## 1. 核心参数详解

在 `config.py` 和 `train.py` 中，以下几个参数决定了训练系统是起飞还是趴窝：

### A. `TRAIN_BATCH_SIZE`: 向显存要速度
*   **含义**: 一次性塞给 GPU 进行并行计算的图片数量。
*   **如何影响性能**:
    *   **太小 (如 32)**: GPU 会频繁启动/停止计算，大量时间浪费在内核启动（Kernel Launch）开销上。GPU 利用率低，显存浪费。
    *   **太大 (如 1024)**: 会撑爆显存 (CUDA Out of Memory)。
    *   **合适**: 在显存不溢出的前提下，**越大越好**。
*   **本次实践**: 从 32 -> 1024 (OOM) -> 400 (最终稳定)。这是提升 GPU 利用率最直接的手段。

### B. `NUM_WORKERS`: 压榨 CPU 备菜
*   **含义**: PyTorch 用于在后台加载和预处理（Augmentation）数据的子进程数量。
*   **比喻**: GPU 是大胃王（吃得极快），CPU 是厨师（切菜、洗菜）。如果厨师太少，大胃王就得停下来等上菜。
*   **如何调优**:
    *   **原则**: $\min(CPU核数, 4 \times GPU数)$。
    *   **误区**: 不是越多越好。开太多会导致 CPU 频繁切换上下文（Context Switching），反而变慢，甚至撑爆内存。
*   **本次实践**: 由于 CPU 极其强大 (25核)，我们将 Workers 从 8 -> 16 -> 24，让 CPU 拼命干活，确保 GPU 从不等待。

### C. `pin_memory=True`: 开通内存高速路
*   **含义**: 在系统内存（RAM）中锁定一块区域，专用于与显存（VRAM）传输数据。
*   **作用**: 避免了 CPU 参与从普通内存到显存的拷贝过程，利用 DMA 技术，传输速度翻倍。
*   **结论**: **只要是 GPU 训练，无脑开启**。

### D. `prefetch_factor`: 提前囤货
*   **含义**: 让每个 Worker 提前准备好多少个 Batch 的数据存着。
*   **作用**: 进一步消除 GPU 的等待间隙。空间换时间。
*   **本次实践**: 设置为 4，稍微多占用一点内存（反正 90GB 用不完），换取极度平滑的数据流。

---

## 2. 软硬件配合：流水线（Pipeline）思维

AI 训练本质上是一条工业流水线。我们的目标是消除瓶颈（Bottleneck）。

**流程图**:
`硬盘 -> (IO) -> 内存 -> (CPU预处理) -> 内存缓冲区 -> (PCIe带宽) -> 显存 -> (GPU计算) -> 梯度反传`

### 调优过程回顾

| 阶段 | 现象 (症状) | 瓶颈分析 (病因) | 处方 (Action) |
| :--- | :--- | :--- | :--- |
| **初始** | 9分钟/Epoch, GPU 利用率低 | **Batch Size 太小**。GPU 算完一点点数据就停下来发呆。 | 增大 Batch Size (32 -> 400)。 |
| **中期** | GPU 利用率在 70%-100% 跳动 | **CPU 供货不足**。GPU 胃口大开了，但 CPU "做菜" 慢了，或者传菜慢了。 | 增加 `NUM_WORKERS` (8 -> 24)。 |
| **终局** | 5分30秒/Epoch, GPU 95%+ | **数据传输曾是隐形瓶颈**。 | 开启 `pin_memory=True` 和 `prefetch_factor`，打通内存到显存的高速路。 |
| **避坑** | 显存/内存暴涨，训练卡死 | **编译开销过大**。`torch.compile` 对动态多分支网络优化副作用明显。 | 移除 `torch.compile`，回退到原生 PyTorch 高效模式。 |

---

## 3. 经验总结 (Rule of Thumb)

1.  **先看显存**: 把 Batch Size 加到显存占用的 80%-90%。
2.  **再看 GPU 利用率**:
    *   如果一直是 0% -> 100% -> 0% 跳变：说明数据加载慢 -> **加 Workers** 或 **开 pin_memory**。
    *   如果不跳变但一直在 60% 左右：说明 Batch Size 还是太小，或者模型计算本身就是无法并行的（很少见）。
3.  **最后看 CPU/内存**: 只要 CPU 不满载 (100% per core)，内存不 OOM，就无视它。AI 训练里 CPU 的宿命就是为 GPU 服务的耗材。

**现在你的配置（Batch 400 + Worker 24 + Pin Memory）已经是这台服务器上在该项目下的"完美形态"。** 享受极速训练吧！
